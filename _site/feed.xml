<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-11-07T13:16:54+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Blog de Yasser </title><subtitle>Explorations en mathématiques, statistiques et IA appliquées aux sciences du vivant</subtitle><author><name>Yasser EL Kaouni</name></author><entry><title type="html">Transformer Series: Introduction</title><link href="http://localhost:4000/deep-learning/2024/09/27/transformer-series-introduction.html" rel="alternate" type="text/html" title="Transformer Series: Introduction" /><published>2024-09-27T11:00:00+02:00</published><updated>2024-09-27T11:00:00+02:00</updated><id>http://localhost:4000/deep-learning/2024/09/27/transformer-series-introduction</id><content type="html" xml:base="http://localhost:4000/deep-learning/2024/09/27/transformer-series-introduction.html"><![CDATA[<p><img src="/assets/images/images_posts/Transformer_series/the_road_to_transformers.png" alt="The Road to Transformers" /></p>

<p>J’entame une série d’articles sur l’architecture des <strong>transformers</strong>. Aujourd’hui, ce paradigme dominant en <strong>apprentissage profond</strong> est omniprésent dans tous les aspects structurant notre rapport à la technologie et à l’intelligence artificielle. <strong>Inventé en 2017</strong> avec le papier <em>“Attention Is All You Need”</em><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, les transformers soutiennent les systèmes de recommandation des plateformes que nous utilisons au quotidien. Ils sont à la base des percées récentes en <strong>traduction automatique</strong> (comme <strong>DeepL</strong>) et révolutionnent même la recherche fondamentale en <strong>biotechnologie</strong>, avec des avancées telles qu’<strong>AlphaFold</strong>.</p>

<p>Il est donc absolument nécessaire de consacrer cette série d’articles à l’étude approfondie de cette architecture, toujours trop peu abordée dans les <strong>cursus académiques en France</strong>.</p>

<h2 id="plan-de-la-série">Plan de la série</h2>

<h3 id="1-des-rnn-aux-mécanismes-dattention-à-venir">1. <a href="#">Des RNN aux mécanismes d’attention</a> <em>(à venir)</em></h3>
<p>Dans ce premier article, nous reviendrons sur l’évolution des réseaux de neurones récurrents (RNN) vers les mécanismes d’attention qui forment la base des transformers.</p>

<h3 id="2-attention-is-all-you-need-à-venir">2. <a href="#">Attention Is All You Need</a> <em>(à venir)</em></h3>
<p>Nous plongerons dans l’article fondateur de 2017, en analysant ses idées révolutionnaires et la manière dont il a redéfini le domaine du <strong>NLP</strong> (Natural Language Processing).</p>

<h3 id="3-les-transformers-aujourdhui-et-demain-à-venir">3. <a href="#">Les Transformers aujourd’hui et demain</a> <em>(à venir)</em></h3>
<p>Exploration des applications modernes des transformers, de <strong>DeepL</strong> à <strong>GPT-2</strong>, et des futurs développements dans des domaines comme la vision par ordinateur avec <strong>ViT</strong>.</p>

<h3 id="4-lentraînement-des-transformers-à-venir">4. <a href="#">L’entraînement des Transformers</a> <em>(à venir)</em></h3>
<p>Un tutoriel pratique sur l’entraînement de votre propre modèle transformer à partir de zéro.</p>

<h3 id="5-deux-variantes-populaires--bert-et-gpt-à-venir">5. <a href="#">Deux variantes populaires : BERT et GPT</a> <em>(à venir)</em></h3>
<p>Focus sur deux des modèles les plus influents issus de l’architecture transformer : <strong>BERT</strong> (Bidirectional Encoder Representations from Transformers) et <strong>GPT</strong> (Generative Pre-trained Transformer).</p>

<h3 id="6-quelques-aspects-de-lalgorithme-dalphafold-à-venir">6. <a href="#">Quelques aspects de l’algorithme d’AlphaFold</a> <em>(à venir)</em></h3>
<p>Discussion sur l’impact d’<strong>AlphaFold</strong> dans la biotechnologie et comment les transformers ont permis de résoudre des problèmes complexes dans la prédiction des structures protéiques.</p>

<h2 id="notes-de-bas-de-page">Notes de bas de page</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). <em>Attention is all you need.</em> In Advances in Neural Information Processing Systems (pp. 5998-6008). <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Yasser EL Kaouni</name></author><category term="deep-learning" /><category term="Transformers" /><category term="NLP" /><category term="AI" /><summary type="html"><![CDATA[]]></summary></entry></feed>